import os
import time
import joblib
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# ---------------------------------------------------------------------
# CONFIGURATION
# ---------------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parents[2]
FEATURE_DIR = BASE_DIR / "data" / "features"
MODEL_DIR = BASE_DIR / "osrs_flipper_ai" / "models" / "trained_models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# ---------------------------------------------------------------------
# LOAD FEATURES (no recomputation)
# ---------------------------------------------------------------------
def load_latest_features():
    """Load the latest feature snapshot (historical or live)."""
    feature_files = sorted(FEATURE_DIR.glob("features_*.parquet"), key=os.path.getmtime, reverse=True)
    if not feature_files:
        raise FileNotFoundError(f"âŒ No feature snapshots found in {FEATURE_DIR}")
    latest_file = feature_files[0]
    print(f"ðŸ“Š Using feature file: {latest_file}")
    return pd.read_parquet(latest_file)

# ---------------------------------------------------------------------
# TRAIN MODEL
# ---------------------------------------------------------------------
def train_model(df: pd.DataFrame):
    """Train a GradientBoostingRegressor using precomputed features only."""
    df = df.replace([np.inf, -np.inf], np.nan)
    df = df.dropna(subset=["mid_price"], how="any")

    # Feature columns used for model input
    feature_cols = [
    "spread_ratio",
    "volatility_1h",
    "rsi_norm",
    "roc_norm",
    "macd_norm",
    "technical_score",
]

    # keep only columns that exist and have >0 non-null values
    available = [c for c in feature_cols if c in df.columns and df[c].notna().sum() > 0]
    if len(available) < 2:
        raise ValueError(
            f"âŒ Not enough valid feature columns for training. "
            f"Found: {available}, Expected: {feature_cols}"
        )

    df = df.dropna(subset=available)
    print(f"âœ… Using {len(df):,} rows with features: {available}")

    X = df[available]
    y = df["spread_ratio"]

    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Model training
    model = GradientBoostingRegressor(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=4,
        random_state=42
    )
    model.fit(X_train, y_train)

    # Evaluate
    preds = model.predict(X_test)
    r2 = r2_score(y_test, preds)
    mae = mean_absolute_error(y_test, preds)

    print(f"âœ… Training complete â€” RÂ²={r2:.4f}, MAE={mae:.6f}")
    return model, available, r2, mae

# ---------------------------------------------------------------------
# MAIN
# ---------------------------------------------------------------------
if __name__ == "__main__":
    print("ðŸš€ Starting model training...")
    df = load_latest_features()
    print(f"ðŸ“¦ Loaded {len(df):,} rows of precomputed features")

    model, features, r2, mae = train_model(df)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    model_dict = {
        "model": model,
        "features": features,
        "r2": r2,
        "mae": mae,
        "timestamp": timestamp,
    }

    model_path = MODEL_DIR / "latest_model.pkl"
    joblib.dump(model_dict, model_path)
    print(f"ðŸ’¾ Saved model â†’ {model_path}")
